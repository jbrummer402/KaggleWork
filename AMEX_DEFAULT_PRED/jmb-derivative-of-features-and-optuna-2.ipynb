{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Credits:** Based on [this](https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7963) amazing notebook.\n",
    "\n",
    "OK. Maybe not **all** you need.\n",
    "<br>\n",
    "But they improve `LightGBM`!\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jackm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from typing_extensions import Self\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import ListedColormap\n",
    "from cycler import cycler\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import scipy.stats\n",
    "import warnings\n",
    "from colorama import Fore, Back, Style\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from lightgbm import LGBMClassifier, log_evaluation\n",
    "\n",
    "plt.rcParams['axes.facecolor'] = '#0057b8'  # blue\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=['#ffd700'] +\n",
    "                                         plt.rcParams['axes.prop_cycle'].by_key()['color'][1:])\n",
    "plt.rcParams['text.color'] = 'w'\n",
    "\n",
    "# Add\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# RUN Parameters\n",
    "###\n",
    "\n",
    "INFERENCE = True  # True for LOCAL RUN   + needed for DART processing\n",
    "\n",
    "ONLY_FIRST_FOLD = False  # False for LOCAL RUN\n",
    "\n",
    "USE_ALL_FEATURES = False  # True for LOCAL RUN\n",
    "\n",
    "USE_LGB_WITH_DART = True  # DART will make training run VERY SLOWLY\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "# Select one of: 'last_set' or 'top_impt_set' or 'minimal_set'\n",
    "IF_NOT_USE_ALL_FEATURES_THEN_USE = 'last_set'\n",
    "\n",
    "# The data contains 1-13 monthly data profiles for each customer. We will use the first, last and nth1 and nth2 for modelling.\n",
    "# Valid values for the next two variables are 0-12 where 0 is the first monthly data profile and 12 is the last (the 13th).\n",
    "# IMPT: If the requested nth data profile does not exist for a customer then the \"last\" data profile will be used instead.\n",
    "# IMPT: Use negative number references to have FEWER missed values for customers with less than 13 monthly data profiles.\n",
    "# The selected monthly profile to be used for nth1 (was 4, -3, -9=4)\n",
    "VALUE_FOR_MONTH_NTH1 = -9\n",
    "# The selected monthly profile to be used for nth2 (was 8, -2, -5=8)\n",
    "VALUE_FOR_MONTH_NTH2 = -5\n",
    "# If True then include \"divide by last\" (first/last, nth1/last, nth2/last) features\n",
    "PCT_CALCULATION_DIVIDE_BY_LAST = True\n",
    "# If True then include \"divide by next\" (first/nth1, nth1/nth2, nth2/last) features\n",
    "PCT_CALCULATION_DIVIDE_BY_NEXT = False\n",
    "\n",
    "DROP_SOME_TRAIN_DATA = False  # Causes errors - not yet working\n",
    "\n",
    "# FILL NAN VALUE\n",
    "NAN_VALUE = 0  # WAS -127\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Failed to open local file '../input/amex-data-integer-dtypes-parquet-format/train.parquet'. Detail: [Windows error 3] The system cannot find the path specified.\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m N_VALID_EXAMPLES \u001b[39m=\u001b[39m BATCHSIZE \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Added\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m pf \u001b[39m=\u001b[39m ParquetFile(\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../input/amex-data-integer-dtypes-parquet-format/train.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m first_ten_rows \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(pf\u001b[39m.\u001b[39miter_batches(batch_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W4sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m df \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_batches([first_ten_rows])\u001b[39m.\u001b[39mto_pandas()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\parquet\\__init__.py:277\u001b[0m, in \u001b[0;36mParquetFile.__init__\u001b[1;34m(self, source, metadata, common_metadata, read_dictionary, memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit, decryption_properties)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, source, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, common_metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m              read_dictionary\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, memory_map\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, buffer_size\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m    274\u001b[0m              pre_buffer\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, coerce_int96_timestamp_unit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m              decryption_properties\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    276\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader \u001b[39m=\u001b[39m ParquetReader()\n\u001b[1;32m--> 277\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreader\u001b[39m.\u001b[39;49mopen(\n\u001b[0;32m    278\u001b[0m         source, use_memory_map\u001b[39m=\u001b[39;49mmemory_map,\n\u001b[0;32m    279\u001b[0m         buffer_size\u001b[39m=\u001b[39;49mbuffer_size, pre_buffer\u001b[39m=\u001b[39;49mpre_buffer,\n\u001b[0;32m    280\u001b[0m         read_dictionary\u001b[39m=\u001b[39;49mread_dictionary, metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[0;32m    281\u001b[0m         coerce_int96_timestamp_unit\u001b[39m=\u001b[39;49mcoerce_int96_timestamp_unit,\n\u001b[0;32m    282\u001b[0m         decryption_properties\u001b[39m=\u001b[39;49mdecryption_properties\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommon_metadata \u001b[39m=\u001b[39m common_metadata\n\u001b[0;32m    285\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nested_paths_by_prefix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_nested_paths()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\_parquet.pyx:1211\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetReader.open\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\io.pxi:1659\u001b[0m, in \u001b[0;36mpyarrow.lib.get_reader\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\io.pxi:1650\u001b[0m, in \u001b[0;36mpyarrow.lib.get_native_file\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\io.pxi:928\u001b[0m, in \u001b[0;36mpyarrow.lib.OSFile.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\io.pxi:938\u001b[0m, in \u001b[0;36mpyarrow.lib.OSFile._open_readable\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\error.pxi:111\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Failed to open local file '../input/amex-data-integer-dtypes-parquet-format/train.parquet'. Detail: [Windows error 3] The system cannot find the path specified.\r\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import thop\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torchvision\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "# max_depth = range(3,12)\n",
    "\n",
    "\n",
    "# Num_leaves should be maximum of 2^(max_depth)\n",
    "\n",
    "# min)data_in_leaf-> number of observations that fir the decision\n",
    "# criteria\n",
    "\n",
    "# n_estimators -> number of decision trees\n",
    "# learning_rate -> step size param of gradient descent\n",
    "\n",
    "\n",
    "DIR = \"..\"\n",
    "BATCHSIZE = 128\n",
    "N_TRAIN_EXAMPLES = BATCHSIZE * 30\n",
    "N_VALID_EXAMPLES = BATCHSIZE * 10\n",
    "\n",
    "\n",
    "# Added\n",
    "\n",
    "\n",
    "pf = ParquetFile(\n",
    "    f'../input/amex-data-integer-dtypes-parquet-format/train.parquet')\n",
    "first_ten_rows = next(pf.iter_batches(batch_size=10))\n",
    "df = pa.Table.from_batches([first_ten_rows]).to_pandas()\n",
    "\n",
    "# These are not used in original code below\n",
    "df.drop(columns=['customer_ID', 'S_2'], inplace=True)\n",
    "all_features = df.columns\n",
    "print(str(len(all_features)))\n",
    "print(str(all_features))\n",
    "\n",
    "cat_cols = ['Balance 30', 'Balance 38', 'Delinquency 63', 'Delinquency 64', 'Delinquency 66', 'Delinquency 68',\n",
    "            'Delinquency 114', 'Delinquency 116', 'Delinquency 117', 'Delinquency 120', 'Delinquency 126', 'Target']\n",
    "cols = [col for col in df.columns if (\n",
    "    col.startswith(('B'))) & (col not in cat_cols[:-1])]\n",
    "# DROP R_ cols TO SAVE RAM for this test\n",
    "df.drop(columns=cols, inplace=True)\n",
    "\n",
    "all_features = df.columns\n",
    "print(str(len(all_features)))\n",
    "print(str(all_features))\n",
    "\n",
    "del pf, first_ten_rows, df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @yunchonggan's fast metric implementation\n",
    "# From https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\n",
    "def amex_metric(y_true: np.array, y_pred: np.array) -> float:\n",
    "\n",
    "    # count of positives and negatives\n",
    "    n_pos = y_true.sum()\n",
    "    n_neg = y_true.shape[0] - n_pos\n",
    "\n",
    "    # sorting by descring prediction values\n",
    "    indices = np.argsort(y_pred)[::-1]\n",
    "    preds, target = y_pred[indices], y_true[indices]\n",
    "\n",
    "    # filter the top 4% by cumulative row weights\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_filter = cum_norm_weight <= 0.04\n",
    "\n",
    "    # default rate captured at 4%\n",
    "    d = target[four_pct_filter].sum() / n_pos\n",
    "\n",
    "    # weighted gini coefficient\n",
    "    lorentz = (target / n_pos).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    # max weighted gini coefficient\n",
    "    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "    # normalized weighted gini coefficient\n",
    "    g = gini / gini_max\n",
    "\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "def lgb_amex_metric(y_true, y_pred):\n",
    "    \"\"\"The competition metric with lightgbm's calling convention\"\"\"\n",
    "    return ('amex',\n",
    "            amex_metric(y_true, y_pred),\n",
    "            True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/amex-default-prediction/train_labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cat_features \u001b[39m=\u001b[39m [\n\u001b[0;32m      <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mB_30\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mB_38\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mD_68\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m ]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m target \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39m../input/amex-default-prediction/train_labels.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mtarget\u001b[39m.\u001b[39mvalues\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape of target: \u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m     f,\n\u001b[0;32m   1219\u001b[0m     mode,\n\u001b[0;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1226\u001b[0m )\n\u001b[0;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/amex-default-prediction/train_labels.csv'"
     ]
    }
   ],
   "source": [
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "\n",
    "target = pd.read_csv(\n",
    "    '../input/amex-default-prediction/train_labels.csv').target.values\n",
    "\n",
    "print(f\"shape of target: {target.shape}\")\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "train = pd.read_parquet(\n",
    "    f'../input/amex-data-integer-dtypes-parquet-format/train.parquet')\n",
    "\n",
    "print(f\"shape of training data: {train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature sets...\n",
      "features_avg: 140\n",
      "features_min: 63\n",
      "features_max: 124\n",
      "features_last: 142\n",
      "features_last_PATRIAL: 63\n",
      "features_top_impt: 30\n",
      "features_minimal: 15\n",
      "features_tiny: 5\n",
      "\n",
      "IF_NOT_USE_ALL_FEATURES_THEN_USE: last_set\n",
      "142\n",
      "['B_1', 'B_2', 'B_3', 'B_4', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_26', 'B_28', 'B_29', 'B_30', 'B_32', 'B_33', 'B_36', 'B_37', 'B_38', 'B_39', 'B_40', 'B_41', 'B_42', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_63', 'D_64', 'D_65', 'D_69', 'D_70', 'D_71', 'D_72', 'D_73', 'D_75', 'D_76', 'D_77', 'D_78', 'D_79', 'D_80', 'D_81', 'D_82', 'D_83', 'D_86', 'D_91', 'D_96', 'D_105', 'D_106', 'D_112', 'D_114', 'D_119', 'D_120', 'D_121', 'D_122', 'D_124', 'D_125', 'D_126', 'D_127', 'D_130', 'D_131', 'D_132', 'D_133', 'D_134', 'D_138', 'D_140', 'D_141', 'D_142', 'D_145', 'P_2', 'P_3', 'P_4', 'R_1', 'R_2', 'R_3', 'R_4', 'R_5', 'R_6', 'R_7', 'R_8', 'R_9', 'R_10', 'R_11', 'R_12', 'R_13', 'R_14', 'R_15', 'R_19', 'R_20', 'R_26', 'R_27', 'S_3', 'S_5', 'S_6', 'S_7', 'S_8', 'S_9', 'S_11', 'S_12', 'S_13', 'S_16', 'S_19', 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27']\n",
      "\n",
      "Revised features sets...\n",
      "142\n",
      "142\n",
      "142\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "# ORIG\n",
    "print('Original feature sets...')\n",
    "features_avg = ['B_1', 'B_2', 'B_3', 'B_4', 'B_5', 'B_6', 'B_8', 'B_9', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18',\n",
    "                'B_19', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_28', 'B_29', 'B_30', 'B_32', 'B_33', 'B_37', 'B_38', 'B_39', 'B_40',\n",
    "                'B_41', 'B_42', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_50', 'D_51', 'D_53', 'D_54', 'D_55',\n",
    "                'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_65', 'D_66', 'D_69', 'D_70', 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_76', 'D_77',\n",
    "                'D_78', 'D_80', 'D_82', 'D_84', 'D_86', 'D_91', 'D_92', 'D_94', 'D_96', 'D_103', 'D_104', 'D_108', 'D_112', 'D_113', 'D_114',\n",
    "                'D_115', 'D_117', 'D_118', 'D_119', 'D_120', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_126', 'D_128', 'D_129', 'D_131',\n",
    "                'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_140', 'D_141', 'D_142', 'D_144', 'D_145', 'P_2', 'P_3', 'P_4', 'R_1', 'R_2',\n",
    "                'R_3', 'R_7', 'R_8', 'R_9', 'R_10', 'R_11', 'R_14', 'R_15', 'R_16', 'R_17', 'R_20', 'R_21', 'R_22', 'R_24', 'R_26', 'R_27',\n",
    "                'S_3', 'S_5', 'S_6', 'S_7', 'S_9', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_18', 'S_22', 'S_23', 'S_25', 'S_26']\n",
    "\n",
    "features_min = ['B_2', 'B_4', 'B_5', 'B_9', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_19', 'B_20', 'B_28', 'B_29', 'B_33', 'B_36', 'B_42', 'D_39',\n",
    "                'D_41', 'D_42', 'D_45', 'D_46', 'D_48', 'D_50', 'D_51', 'D_53', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_62', 'D_70', 'D_71',\n",
    "                'D_74', 'D_75', 'D_78', 'D_83', 'D_102', 'D_112', 'D_113', 'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_128', 'D_132', 'D_140',\n",
    "                'D_141', 'D_144', 'D_145', 'P_2', 'P_3', 'R_1', 'R_27', 'S_3', 'S_5', 'S_7', 'S_9', 'S_11', 'S_12', 'S_23', 'S_25']\n",
    "\n",
    "features_max = ['B_1', 'B_2', 'B_3', 'B_4', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'B_10', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18',\n",
    "                'B_19', 'B_21', 'B_23', 'B_24', 'B_25', 'B_29', 'B_30', 'B_33', 'B_37', 'B_38', 'B_39', 'B_40', 'B_42', 'D_39', 'D_41', 'D_42',\n",
    "                'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_52', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_63',\n",
    "                'D_64', 'D_65', 'D_70', 'D_71', 'D_72', 'D_73', 'D_74', 'D_76', 'D_77', 'D_78', 'D_80', 'D_82', 'D_84', 'D_91', 'D_102', 'D_105',\n",
    "                'D_107', 'D_110', 'D_111', 'D_112', 'D_115', 'D_116', 'D_117', 'D_118', 'D_119', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125',\n",
    "                'D_126', 'D_128', 'D_131', 'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_138', 'D_140', 'D_141', 'D_142', 'D_144', 'D_145',\n",
    "                'P_2', 'P_3', 'P_4',\n",
    "                'R_1', 'R_3', 'R_5', 'R_6', 'R_7', 'R_8', 'R_10', 'R_11', 'R_14', 'R_17', 'R_20', 'R_26', 'R_27',\n",
    "                'S_3', 'S_5', 'S_7', 'S_8', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27'\n",
    "                ]\n",
    "\n",
    "features_last_PATRIAL = ['B_1', 'B_2', 'B_3', 'B_4', 'B_5', 'B_6', 'B_7', 'B_8',  # 'B_9', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18',\n",
    "                         # 'B_26', 'B_28', 'B_29', 'B_30', 'B_32', 'B_33', 'B_36', 'B_37', 'B_38',\n",
    "                         'B_19', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25',\n",
    "                         # 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51',\n",
    "                         'B_39', 'B_40', 'B_41', 'B_42', 'D_39', 'D_41', 'D_42',\n",
    "                         # 'D_60', 'D_61', 'D_62', 'D_63', 'D_64', 'D_65', 'D_69', 'D_70', 'D_71',\n",
    "                         'D_52', 'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59',\n",
    "                         # 'D_80', 'D_81', 'D_82', 'D_83', 'D_86', 'D_91', 'D_96', 'D_105', 'D_106',\n",
    "                         'D_72', 'D_73', 'D_75', 'D_76', 'D_77', 'D_78', 'D_79',\n",
    "                         # 'D_124', 'D_125', 'D_126', 'D_127', 'D_130', 'D_131', 'D_132', 'D_133',\n",
    "                         'D_112', 'D_114', 'D_119', 'D_120', 'D_121', 'D_122',\n",
    "                         # 'P_3', 'P_4', 'R_1', 'R_2', 'R_3', 'R_4', 'R_5', 'R_6', 'R_7',\n",
    "                         'D_134', 'D_138', 'D_140', 'D_141', 'D_142', 'D_145', 'P_2',\n",
    "                         # 'R_15', 'R_19', 'R_20', 'R_26', 'R_27', 'S_3', 'S_5', 'S_6', 'S_7',\n",
    "                         'R_8', 'R_9', 'R_10', 'R_11', 'R_12', 'R_13', 'R_14',\n",
    "                         # 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27'\n",
    "                         'S_8', 'S_9', 'S_11', 'S_12', 'S_13', 'S_16', 'S_19',\n",
    "                         ]\n",
    "\n",
    "features_last = ['B_1', 'B_2', 'B_3', 'B_4', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18',\n",
    "                 'B_19', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_26', 'B_28', 'B_29', 'B_30', 'B_32', 'B_33', 'B_36', 'B_37', 'B_38',\n",
    "                 'B_39', 'B_40', 'B_41', 'B_42', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51',\n",
    "                 'D_52', 'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_63', 'D_64', 'D_65', 'D_69', 'D_70', 'D_71',\n",
    "                 'D_72', 'D_73', 'D_75', 'D_76', 'D_77', 'D_78', 'D_79', 'D_80', 'D_81', 'D_82', 'D_83', 'D_86', 'D_91', 'D_96', 'D_105', 'D_106',\n",
    "                 'D_112', 'D_114', 'D_119', 'D_120', 'D_121', 'D_122', 'D_124', 'D_125', 'D_126', 'D_127', 'D_130', 'D_131', 'D_132', 'D_133',\n",
    "                 'D_134', 'D_138', 'D_140', 'D_141', 'D_142', 'D_145', 'P_2', 'P_3', 'P_4', 'R_1', 'R_2', 'R_3', 'R_4', 'R_5', 'R_6', 'R_7',\n",
    "                 'R_8', 'R_9', 'R_10', 'R_11', 'R_12', 'R_13', 'R_14', 'R_15', 'R_19', 'R_20', 'R_26', 'R_27', 'S_3', 'S_5', 'S_6', 'S_7',\n",
    "                 'S_8', 'S_9', 'S_11', 'S_12', 'S_13', 'S_16', 'S_19', 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27'\n",
    "                 ]\n",
    "\n",
    "# BASED ON \"gml AMEX Default Prediction EDA & LGBM Baseline\" RUNS USING A SINGLE TYPES OF FEATURES (v6,v14,v13,v15,v12)\n",
    "# If kept to 30 then will allow inference to be run within Kaggle RAM limits\n",
    "features_top_impt = ['D_43', 'D_60', 'D_46', 'D_45', 'D_39', 'D_48', 'D_47', 'D_53', 'D_59', 'D_44',  # 'D_71', 'D_61', 'D_75', 'D_121', 'D_62',\n",
    "                     # 'B_19', 'B_15', 'B_10', 'B_6', 'B_13',\n",
    "                     'B_9', 'B_5', 'B_3', 'B_26', 'B_40', 'B_4', 'B_8',\n",
    "                     'P_2', 'P_3', 'P_4',\n",
    "                     'R_3', 'R_27', 'R_1', 'R_2', 'R_6',  # 'R_10', 'R_5', 'R_21', 'R_20', 'R_7',\n",
    "                     'S_22', 'S_24', 'S_27', 'S_9', 'S_5',  # 'S_3', 'S_7', 'S_13', 'S_23', 'S_12',\n",
    "                     #'_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n",
    "                     ]\n",
    "\n",
    "features_minimal = ['D_43', 'D_60', 'D_46',  # 'D_45', 'D_39', 'D_48', 'D_47', 'D_53', 'D_59', 'D_44', #'D_71', 'D_61', 'D_75', 'D_121', 'D_62',\n",
    "                    # 'B_26', 'B_40', 'B_4', 'B_8', #'B_19', 'B_15', 'B_10', 'B_6', 'B_13',\n",
    "                    'B_9', 'B_5', 'B_3',\n",
    "                    'P_2', 'P_3', 'P_4',\n",
    "                    'R_3', 'R_27', 'R_1',  # 'R_2', 'R_6', #'R_10', 'R_5', 'R_21', 'R_20', 'R_7',\n",
    "                    'S_22', 'S_24', 'S_27',  # 'S_9', 'S_5',# 'S_3', 'S_7', 'S_13', 'S_23', 'S_12',\n",
    "                    #'_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n",
    "                    ]\n",
    "\n",
    "features_tiny = ['D_43',  # 'D_60', 'D_46', #'D_45', 'D_39', 'D_48', 'D_47', 'D_53', 'D_59', 'D_44', #'D_71', 'D_61', 'D_75', 'D_121', 'D_62',\n",
    "                 'B_9',  # 'B_5', 'B_3', #'B_26', 'B_40', 'B_4', 'B_8', #'B_19', 'B_15', 'B_10', 'B_6', 'B_13',\n",
    "                 'P_2',  # 'P_3', 'P_4',\n",
    "                 'R_3',  # 'R_27', 'R_1', #'R_2', 'R_6', #'R_10', 'R_5', 'R_21', 'R_20', 'R_7',\n",
    "                 'S_22',  # 'S_24', 'S_27', #'S_9', 'S_5',# 'S_3', 'S_7', 'S_13', 'S_23', 'S_12',\n",
    "                 #'_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n",
    "                 ]\n",
    "\n",
    "print('features_avg: ' + str(len(features_avg)))\n",
    "print('features_min: ' + str(len(features_min)))\n",
    "print('features_max: ' + str(len(features_max)))\n",
    "print('features_last: ' + str(len(features_last)))\n",
    "print('features_last_PATRIAL: ' + str(len(features_last_PATRIAL)))\n",
    "print('features_top_impt: ' + str(len(features_top_impt)))\n",
    "print('features_minimal: ' + str(len(features_minimal)))\n",
    "print('features_tiny: ' + str(len(features_tiny)))\n",
    "print()\n",
    "\n",
    "# REVISED set of features to be used:\n",
    "# features_to_use = features_last_PATRIAL\n",
    "# features_to_use = features_last\n",
    "# features_to_use = all_features\n",
    "#features_to_use = features_top_impt\n",
    "# features_to_use = features_max\n",
    "if IF_NOT_USE_ALL_FEATURES_THEN_USE == 'last_set':\n",
    "    features_to_use = features_last\n",
    "    print('IF_NOT_USE_ALL_FEATURES_THEN_USE: last_set')\n",
    "elif IF_NOT_USE_ALL_FEATURES_THEN_USE == 'minimal_set':\n",
    "    features_to_use = features_minimal\n",
    "    print('IF_NOT_USE_ALL_FEATURES_THEN_USE: minimal_set')\n",
    "elif IF_NOT_USE_ALL_FEATURES_THEN_USE == 'tiny_set':\n",
    "    features_to_use = features_tiny\n",
    "    print('IF_NOT_USE_ALL_FEATURES_THEN_USE: tiny_set')\n",
    "else:  # Use the smaller 'top_impt_set' by defalult\n",
    "    features_to_use = features_top_impt\n",
    "    print('IF_NOT_USE_ALL_FEATURES_THEN_USE: top_impt_set')\n",
    "\n",
    "print(str(len(features_to_use)))\n",
    "print(str(features_to_use))\n",
    "print()\n",
    "\n",
    "print('Revised features sets...')\n",
    "features_avg = features_to_use\n",
    "features_min = features_to_use\n",
    "features_max = features_to_use\n",
    "features_last = features_to_use\n",
    "print(str(len(features_avg)))\n",
    "print(str(len(features_min)))\n",
    "print(str(len(features_max)))\n",
    "print(str(len(features_last)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5531451\n"
     ]
    }
   ],
   "source": [
    "select_cols = [col for col in train.columns if (col.startswith(('P_2')))]\n",
    "features = [f for f in train.columns if f != 'customer_ID' and f != 'target']\n",
    "print(str(len(select_cols)))\n",
    "train[select_cols]\n",
    "print(len(train[features]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b96f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T18:53:58.944110Z",
     "iopub.status.busy": "2022-07-25T18:53:58.943437Z",
     "iopub.status.idle": "2022-07-25T18:54:00.199451Z",
     "shell.execute_reply": "2022-07-25T18:54:00.198077Z"
    },
    "papermill": {
     "duration": 1.269924,
     "end_time": "2022-07-25T18:54:00.201935",
     "exception": false,
     "start_time": "2022-07-25T18:53:58.932011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n",
      "Index(['P_2', 'D_39', 'B_1', 'B_2', 'R_1', 'S_3', 'D_41', 'B_3', 'D_42',\n",
      "       'D_43',\n",
      "       ...\n",
      "       'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143',\n",
      "       'D_144', 'D_145'],\n",
      "      dtype='object', length=188)\n",
      "148\n",
      "Index(['P_2', 'D_39', 'R_1', 'S_3', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45',\n",
      "       'R_2',\n",
      "       ...\n",
      "       'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143',\n",
      "       'D_144', 'D_145'],\n",
      "      dtype='object', length=148)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Added\n",
    "\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa\n",
    "\n",
    "pf = ParquetFile(\n",
    "    f'../input/amex-data-integer-dtypes-parquet-format/train.parquet')\n",
    "first_ten_rows = next(pf.iter_batches(batch_size=10))\n",
    "df = pa.Table.from_batches([first_ten_rows]).to_pandas()\n",
    "\n",
    "# These are not used in original code below\n",
    "df.drop(columns=['customer_ID', 'S_2'], inplace=True)\n",
    "all_features = df.columns\n",
    "print(str(len(all_features)))\n",
    "print(str(all_features))\n",
    "\n",
    "cat_cols = ['Balance 30', 'Balance 38', 'Delinquency 63', 'Delinquency 64', 'Delinquency 66', 'Delinquency 68',\n",
    "            'Delinquency 114', 'Delinquency 116', 'Delinquency 117', 'Delinquency 120', 'Delinquency 126', 'Target']\n",
    "cols = [col for col in df.columns if (\n",
    "    col.startswith(('B'))) & (col not in cat_cols[:-1])]\n",
    "# DROP R_ cols TO SAVE RAM for this test\n",
    "df.drop(columns=cols, inplace=True)\n",
    "\n",
    "all_features = df.columns\n",
    "print(str(len(all_features)))\n",
    "print(str(all_features))\n",
    "\n",
    "del pf, first_ten_rows, df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process and feature engineering \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPTED FROM ---> https://www.kaggle.com/code/glaskosk/xgboost-starter-0-793?scriptVersionId=101352220\n",
    "\n",
    "from hashlib import new\n",
    "\n",
    "\n",
    "def process_and_feature_engineer(df):\n",
    "    # FEATURE ENGINEERING FROM\n",
    "    # https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n",
    "    all_cols = [c for c in list(df.columns) if c not in ['customer_ID', 'S_2']]\n",
    "    cat_features = [\"B_30\", \"B_38\", \"D_114\", \"D_116\", \"D_117\",\n",
    "                    \"D_120\", \"D_126\", \"D_63\", \"D_64\", \"D_66\", \"D_68\"]\n",
    "    num_features = [col for col in all_cols if col not in cat_features]\n",
    "    print(str(len(num_features)))\n",
    "\n",
    "    # DEFAULT values to use if some below are NaN << [0]=FIRST and [-1]=LAST >>\n",
    "    NO_NAN_df_nth = df.groupby(\"customer_ID\")[num_features].nth([-1])\n",
    "    # REDUNDANT DATA--> NO_NAN_df_nth = df.groupby(\"customer_ID\")[num_features].agg(['last']) ### MAY be BETTER to use last instead of first for missing instances\n",
    "    # REDUNDANT DATA--> NO_NAN_df_nth.columns = ['_'.join(x) for x in NO_NAN_df_nth.columns]  ### INCLUDE this line ONLY IF .agg(['last'])\n",
    "    gc.collect()\n",
    "\n",
    "    print('VALUE_FOR_MONTH_NTH1: ' + str(VALUE_FOR_MONTH_NTH1))\n",
    "    # 0 to N;   #.dropna()  Keep NaNs  e.g. if nth = 15 then all are NaNs\n",
    "    test_df_nth = df.groupby(\"customer_ID\")[\n",
    "        num_features].nth([VALUE_FOR_MONTH_NTH1])\n",
    "    test_df_nth = test_df_nth.combine_first(NO_NAN_df_nth)\n",
    "\n",
    "    test_df_nth_col_names = test_df_nth.columns\n",
    "    test_df_nth_col_names = test_df_nth_col_names + '_nth1'\n",
    "    test_df_nth.columns = test_df_nth_col_names\n",
    "    gc.collect()\n",
    "    print('1) Calculated _nth1')\n",
    "\n",
    "    print('VALUE_FOR_MONTH_NTH2: ' + str(VALUE_FOR_MONTH_NTH2))\n",
    "    # 0 to N;   #.dropna()  Keep NaNs  e.g. if nth = 15 then all are NaNs\n",
    "    test_df_nth2 = df.groupby(\"customer_ID\")[\n",
    "        num_features].nth([VALUE_FOR_MONTH_NTH2])\n",
    "    test_df_nth2 = test_df_nth2.combine_first(NO_NAN_df_nth)\n",
    "    del NO_NAN_df_nth\n",
    "    gc.collect()\n",
    "\n",
    "    test_df_nth2_col_names = test_df_nth2.columns\n",
    "    test_df_nth2_col_names = test_df_nth2_col_names + '_nth2'\n",
    "    test_df_nth2.columns = test_df_nth2_col_names\n",
    "    print('2) Calculated _nth2')\n",
    "\n",
    "#     test_num_agg = df.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last']) ### ORIG\n",
    "    test_num_agg = df.groupby(\"customer_ID\")[num_features].agg(\n",
    "        ['first', 'last', 'min', 'max', 'mean', 'median', 'std'])  # , 'median', 'std'\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    print('3) Calculated test_num_agg (e.g. first,last,min,max,mean,median,std)')\n",
    "\n",
    "    test_num_agg = pd.concat([test_df_nth, test_df_nth2, test_num_agg], axis=1)\n",
    "    del test_df_nth, test_df_nth2\n",
    "    gc.collect()\n",
    "\n",
    "    # Lag Features\n",
    "    for col in test_num_agg:\n",
    "        new_cols = {}\n",
    "        # \"divide by last\" (first/last, nth1/last, nth2/last)\n",
    "        if PCT_CALCULATION_DIVIDE_BY_LAST == True:\n",
    "            # if 'last' in col and col.replace('last', 'first') in test_num_agg:\n",
    "            #test_num_agg[col + '_lag_sub'] = test_num_agg[col] - test_num_agg[col.replace('last', 'first')]\n",
    "            if 'first' in col and col.replace('first', 'last') in test_num_agg:\n",
    "                new_cols[col + 'div_last'] = (test_num_agg[col.replace(\n",
    "                    'first', 'last')] - test_num_agg[col]) / (test_num_agg[col] + 0.000001)\n",
    "                #test_num_agg[col + '_div_last'] = (test_num_agg[col.replace('first', 'last')] - test_num_agg[col]) / (test_num_agg[col] + 0.000001)\n",
    "            if 'nth1' in col and col.replace('nth1', 'last') in test_num_agg:\n",
    "                new_cols[col + '_div_last'] = (test_num_agg[col.replace(\n",
    "                    'nth1', 'last')] - test_num_agg[col]) / (test_num_agg[col] + 0.000001)\n",
    "                #test_num_agg[col + '_div_last'] = (test_num_agg[col.replace('nth1', 'last')] - test_num_agg[col]) / (test_num_agg[col] + 0.000001)\n",
    "            if 'nth2' in col and col.replace('nth2', 'last') in test_num_agg:\n",
    "                new_cols[col + '_div_last'] = (test_num_agg[col.replace(\n",
    "                    'nth2', 'last')] - test_num_agg[col]) / (test_num_agg[col] + 0.000001)\n",
    "                #test_num_agg[col + '_div_last'] = (test_num_agg[col.replace('nth2', 'last')] - test_num_agg[col]) / (test_num_agg[col] + 0.000001)\n",
    "\n",
    "        # \"divide by next\" (first/nth1, nth1/nth2, nth2/last)\n",
    "        if PCT_CALCULATION_DIVIDE_BY_NEXT == True:\n",
    "            if 'first' in col and col.replace('first', 'nth1') in test_num_agg:\n",
    "                new_cols[col + '_div_nth1'] = (test_num_agg[col.replace(\n",
    "                    'first', 'nth1')] - test_num_agg[col]) / (test_num_agg[col] + 0.000001)\n",
    "                #test_num_agg[col + '_div_nth1'] = (test_num_agg[col.replace('first', 'nth1')] - test_num_agg[col]) / (test_num_agg[col] + 0.000001)\n",
    "            if 'nth1' in col and col.replace('nth1', 'nth2') in test_num_agg:\n",
    "                new_cols[col + '_div_nth2'] = (test_num_agg[col.replace(\n",
    "                    'nth1', 'nth2')] - test_num_agg[col]) / (test_num_agg[col] + 0.000001)\n",
    "                #test_num_agg[col + '_div_nth2'] = (test_num_agg[col.replace('nth1', 'nth2')] - test_num_agg[col]) / (test_num_agg[col] + 0.000001)\n",
    "            if 'nth2' in col and col.replace('nth2', 'last') in test_num_agg:\n",
    "                new_cols[col + '_div_last'] = (test_num_agg[col.replace(\n",
    "                    'nth2', 'last')] - test_num_agg[col]) / (test_num_agg[col] + 0.000001)\n",
    "                #test_num_agg[col + '_div_last'] = (test_num_agg[col.replace('nth2', 'last')] - test_num_agg[col]) / (test_num_agg[col] + 0.000001)\n",
    "        test_num_agg.assign(**new_cols)\n",
    "    # SUM Lag features\n",
    "    for col in num_features:\n",
    "        # \"divide by last\" (first/last, nth1/last, nth2/last)\n",
    "        if PCT_CALCULATION_DIVIDE_BY_LAST == True:\n",
    "            new_cols[col + '_divLast_SUM'] = test_num_agg[col + '_first_div_last'] + \\\n",
    "                test_num_agg[col + '_nth1_div_last'] + \\\n",
    "                test_num_agg[col + '_nth2_div_last']\n",
    "            #test_num_agg[col + '_divLast_SUM'] = test_num_agg[col + '_first_div_last'] + test_num_agg[col + '_nth1_div_last'] + test_num_agg[col + '_nth2_div_last']\n",
    "            # >test_num_agg[col + '_avgLast_SUM'] = test_num_agg[col + '_divLast_SUM'] / 3\n",
    "\n",
    "        # \"divide by next\" (first/nth1, nth1/nth2, nth2/last)\n",
    "        if PCT_CALCULATION_DIVIDE_BY_NEXT == True:\n",
    "            # test_num_agg[col + '_divNext_SUM'] = test_num_agg[col + '_first_div_nth1'] + test_num_agg[col + '_nth1_div_nth2'] + test_num_agg[col + '_nth2_div_last']\n",
    "            new_cols[col + '_divNext_SUM'] = test_num_agg[col + '_first_div_nth1'] + \\\n",
    "                test_num_agg[col + '_nth1_div_nth2'] + \\\n",
    "                test_num_agg[col + '_nth2_div_last']\n",
    "            # >test_num_agg[col + '_avgNext_SUM'] = test_num_agg[col + '_divNext_SUM'] / 3\n",
    "        test_num_agg.assign(**new_cols)\n",
    "    print('4) Calculated percent change and SUM of pecent change features (first div last, ...)')\n",
    "    gc.collect()\n",
    "\n",
    "#     test_cat_agg = df.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])   ### ORIG\n",
    "    # TRY other options\n",
    "    test_cat_agg = df.groupby(\"customer_ID\")[cat_features].agg(\n",
    "        ['count', 'last', 'nunique', 'first'])  # TRY other options\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    print('5) Calculated test_cat_agg (e.g. count,last,nunique,first)')\n",
    "\n",
    "    # >df = pd.concat([test_num_agg, test_cat_agg], axis=1)   ### ORIG\n",
    "    df = pd.concat([test_num_agg, test_cat_agg], axis=1)\n",
    "\n",
    "    # >del test_num_agg, test_cat_agg   ### ORIG\n",
    "    del test_num_agg, test_cat_agg  # ORIG\n",
    "    gc.collect()\n",
    "    print('Shape after engineering', df.shape)\n",
    "\n",
    "    #Num_Nan = df.isna().sum().sum()\n",
    "    #print('Num_Nan: ' + str(Num_Nan))\n",
    "    df = df.fillna(NAN_VALUE)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f50e8aa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T18:54:00.340613Z",
     "iopub.status.busy": "2022-07-25T18:54:00.339726Z",
     "iopub.status.idle": "2022-07-25T18:56:14.047813Z",
     "shell.execute_reply": "2022-07-25T18:56:14.046461Z"
    },
    "papermill": {
     "duration": 133.722323,
     "end_time": "2022-07-25T18:56:14.050388",
     "exception": false,
     "start_time": "2022-07-25T18:54:00.328065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading test\n",
      "\n",
      "USE SELECT FEATURES (cols): \n",
      "orig num cols: 190\n",
      "NEW num cols: 148\n",
      "135\n",
      "VALUE_FOR_MONTH_NTH1: -9\n",
      "1) Calculated _nth1\n",
      "VALUE_FOR_MONTH_NTH2: -5\n",
      "2) Calculated _nth2\n",
      "3) Calculated test_num_agg (e.g. first,last,min,max,mean,median,std)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'P_2_first_div_last'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'P_2_first_div_last'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:33\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb Cell 13\u001b[0m in \u001b[0;36mprocess_and_feature_engineer\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#X33sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m num_features:\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#X33sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39mif\u001b[39;00m PCT_CALCULATION_DIVIDE_BY_LAST \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:   \u001b[39m### \"divide by last\" (first/last, nth1/last, nth2/last)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#X33sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m         new_cols[col \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_divLast_SUM\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m test_num_agg[col \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m_first_div_last\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m+\u001b[39m test_num_agg[col \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_nth1_div_last\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m test_num_agg[col \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_nth2_div_last\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#X33sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m         \u001b[39m#test_num_agg[col + '_divLast_SUM'] = test_num_agg[col + '_first_div_last'] + test_num_agg[col + '_nth1_div_last'] + test_num_agg[col + '_nth2_div_last']\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#X33sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m         \u001b[39m#>test_num_agg[col + '_avgLast_SUM'] = test_num_agg[col + '_divLast_SUM'] / 3\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#X33sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     \u001b[39mif\u001b[39;00m PCT_CALCULATION_DIVIDE_BY_NEXT \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:   \u001b[39m### \"divide by next\" (first/nth1, nth1/nth2, nth2/last)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/jackbrummer/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-derivative-of-features-and-optuna-2.ipynb#X33sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m         \u001b[39m# test_num_agg[col + '_divNext_SUM'] = test_num_agg[col + '_first_div_nth1'] + test_num_agg[col + '_nth1_div_nth2'] + test_num_agg[col + '_nth2_div_last']\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'P_2_first_div_last'"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "for i in ['test', 'train'] if INFERENCE else ['train']:\n",
    "    print()\n",
    "    print('Loading ' + str(i))\n",
    "    print()\n",
    "\n",
    "    df = pd.read_parquet(\n",
    "        f'../input/amex-data-integer-dtypes-parquet-format/{i}.parquet')\n",
    "\n",
    "    if i == 'train' and DROP_SOME_TRAIN_DATA == True:\n",
    "        print('DROP_SOME_TRAIN_DATA')\n",
    "        orig_length = len(df)\n",
    "        print(str(orig_length))\n",
    "        cutoff_point = orig_length // 2  # Drop the first half of the data\n",
    "        print(str(cutoff_point))\n",
    "        df = df.iloc[cutoff_point:]\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    if USE_ALL_FEATURES == True:\n",
    "        print('USE ALL FEATUTRES (cols): ' + str(len(df.columns)))\n",
    "    else:\n",
    "        # Just use SELECT features (not enough RAM for all features)\n",
    "        print('USE SELECT FEATURES (cols): ')\n",
    "        cat_features = [\"B_30\", \"B_38\", \"D_114\", \"D_116\", \"D_117\",\n",
    "                        \"D_120\", \"D_126\", \"D_63\", \"D_64\", \"D_66\", \"D_68\"]\n",
    "        cat_cols = ['Balance 30', 'Balance 38', 'Delinquency 63', 'Delinquency 64', 'Delinquency 66', 'Delinquency 68',\n",
    "                    'Delinquency 114', 'Delinquency 116', 'Delinquency 117', 'Delinquency 120', 'Delinquency 126', 'Target']\n",
    "        cols_to_keep = features_to_use + cat_cols + \\\n",
    "            cat_features + ['customer_ID', 'S_2']\n",
    "        cols_to_drop = [col for col in df.columns if (col not in cols_to_keep)]\n",
    "        # DROP R_ cols TO SAVE RAM for this test\n",
    "        print('orig num cols: ' + str(len(df.columns)))\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "        gc.collect()\n",
    "        print('NEW num cols: ' + str(len(df.columns)))\n",
    "\n",
    "    df = process_and_feature_engineer(df)\n",
    "\n",
    "    if i == 'train':\n",
    "        train = df\n",
    "    else:\n",
    "        test = df\n",
    "    print(f\" Final {i} shape: {df.shape}\")\n",
    "\n",
    "    del df  # , df_avg, df_min, df_max, cid, last\n",
    "    gc.collect()\n",
    "\n",
    "# # Load dataset\n",
    "# train_data = tf.data.experimental.make_csv_dataset(\n",
    "#     'train.csv',\n",
    "#     batch_size=32,\n",
    "#     label_name='label',\n",
    "#     num_epochs=1)\n",
    "\n",
    "# # Train LightGBM model\n",
    "# model = lgb.LGBMModel()\n",
    "# model.fit(train_data, eval_set=validation_data, ...)\n",
    "\n",
    "# This lets you leverage TensorFlow for reading and batching data, while using LightGBM for the actual training of the model. Hope that helps!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% % time\n",
    "# Cross-validation of the classifier\n",
    "\n",
    "# - - - - - lgb approach...\n",
    "# Label encode categorical features\n",
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "for cat_col in cat_features:\n",
    "    encoder = LabelEncoder()\n",
    "    train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "    test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "if USE_LGB_WITH_DART == True:\n",
    "    print('Training with DART')\n",
    "else:\n",
    "    print('Training WITHOUT DART')\n",
    "# - - - - -\n",
    "\n",
    "features = [f for f in train.columns if f != 'customer_ID' and f != 'target']\n",
    "\n",
    "\n",
    "def my_booster(random_state=1, n_estimators=1200):\n",
    "    return LGBMClassifier(n_estimators=n_estimators,\n",
    "                          learning_rate=0.03, reg_lambda=50,\n",
    "                          min_child_samples=2400,\n",
    "                          num_leaves=95,\n",
    "                          colsample_bytree=0.19,\n",
    "                          max_bins=511, random_state=random_state)\n",
    "\n",
    "\n",
    "print(f\"{len(features)} features\")\n",
    "score_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "ft_importance = pd.DataFrame(index=train.columns)  # ADD\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "# kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "# for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, target)): ### ORIG train[CFG.target])):\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(train, target)):\n",
    "    X_tr, X_va, y_tr, y_va, model = None, None, None, None, None\n",
    "    start_time = datetime.datetime.now()\n",
    "    X_tr = train.iloc[idx_tr][features]\n",
    "    X_va = train.iloc[idx_va][features]\n",
    "    y_tr = target[idx_tr]\n",
    "    y_va = target[idx_va]\n",
    "\n",
    "    ### model = my_booster()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore', category=UserWarning)\n",
    "        # - - - - - lgb approach...\n",
    "#         x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "#         y_train, y_val = target[CFG.target].iloc[trn_ind], target[CFG.target].iloc[val_ind]\n",
    "        x_train = train.iloc[idx_tr][features]\n",
    "        x_val = train.iloc[idx_va][features]\n",
    "        y_train = target[idx_tr]\n",
    "        y_val = target[idx_va]\n",
    "\n",
    "        lgb_train = lgb.Dataset(\n",
    "            x_train, y_train, categorical_feature=cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "        model = lgb.train(\n",
    "            params=params,\n",
    "            train_set=lgb_train,\n",
    "            num_boost_round=1500,  # WAS 10500\n",
    "            valid_sets=[lgb_train, lgb_valid],\n",
    "            early_stopping_rounds=100,  # WAS 100\n",
    "            verbose_eval=200,\n",
    "            feval=lgb_amex_metric\n",
    "        )\n",
    "        gc.collect()\n",
    "        # - - - - -\n",
    "#         model.fit(X_tr, y_tr,\n",
    "#                   eval_set = [(X_va, y_va)],\n",
    "#                   eval_metric=[lgb_amex_metric],\n",
    "#                   callbacks=[log_evaluation(100)])\n",
    "\n",
    "    X_tr, y_tr = None, None\n",
    "    # ORIG   y_va_pred = model.predict_proba(X_va, raw_score=True)\n",
    "    y_va_pred = model.predict(X_va, raw_score=True)\n",
    "    score = amex_metric(y_va, y_va_pred)\n",
    "    # >n_trees = model.best_iteration_\n",
    "    #if n_trees is None: n_trees = model.n_estimators\n",
    "    n_trees = 0\n",
    "    print(f\"{Fore.GREEN}{Style.BRIGHT}Fold {fold} | {str(datetime.datetime.now() - start_time)[-12:-7]} |\"\n",
    "          f\" {n_trees:5} trees |\"\n",
    "          f\"                Score = {score:.5f}{Style.RESET_ALL}\")\n",
    "    score_list.append(score)\n",
    "    # ft_importance[\"Importance_Fold\"+str(fold)]=model.feature_importances_   ### ADD\n",
    "\n",
    "    if INFERENCE:\n",
    "        # ORIG   y_pred_list.append(model.predict_proba(test[features], raw_score=True))\n",
    "        y_pred_list.append(model.predict(test[features], raw_score=True))\n",
    "\n",
    "    if ONLY_FIRST_FOLD:  # we only want the first fold\n",
    "        print('RUN FIRST FOLD ONLY')\n",
    "        break\n",
    "    gc.collect()\n",
    "\n",
    "gc.collect()\n",
    "print(f\"{Fore.GREEN}{Style.BRIGHT}OOF Score:                       {np.mean(score_list):.5f}{Style.RESET_ALL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative of features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import skip\n",
    "import numpy as np\n",
    "import pdb\n",
    "import traceback\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "gc.collect()\n",
    "train[features].drop('S_2', axis=1)\n",
    "# compute gradient of each feature\n",
    "# pdb.set_trace()\n",
    "try:\n",
    "    gradients = np.gradient(train[features], axis=1)\n",
    "    scores = mutual_info_classif(gradients.T, target)\n",
    "except TypeError as e:\n",
    "    print(e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "m_infos = scores[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import trial, study, create_study\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostClassifier\n",
    "from itertools import combinations\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import random\n",
    "import warnings\n",
    "import itertools\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "class IdealModel:\n",
    "    # Implement model type and assign score to each\n",
    "    def __init__(self, target, folds=5, input_dir='./input/amex-fe'):\n",
    "\n",
    "        self.folds = folds\n",
    "        self.input_dir = input_dir\n",
    "        self.target = target\n",
    "\n",
    "    def objective_cross(self, trial):\n",
    "        cv = StratifiedKFold(n_splits=self.folds,\n",
    "                             shuffle=True, random_state=1121218)\n",
    "\n",
    "        cv_scores = np.empty(cv.n_splits)\n",
    "        for fold_idx, (idx_tr, idx_va) in enumerate(cv.split(range(len(train)))):\n",
    "            train_data, valid_data = train.iloc[idx_tr][features], train.iloc[idx_va][features]\n",
    "            y_train, y_valid = self.target[idx_tr], self.target[idx_va]\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore', category=UserWarning)\n",
    "                # - - - - - lgb approach...\n",
    "        #         x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        #         y_train, y_val = target[CFG.target].iloc[trn_ind], target[CFG.target].iloc[val_ind]\n",
    "                x_train = train.iloc[idx_tr][features]\n",
    "                x_val = train.iloc[idx_va][features]\n",
    "                y_train = target[idx_tr]\n",
    "                y_val = target[idx_va]\n",
    "\n",
    "                lgb_train = lgb.Dataset(\n",
    "                    x_train, y_train, categorical_feature=cat_features)\n",
    "                lgb_valid = lgb.Dataset(\n",
    "                    x_val, y_val, categorical_feature=cat_features)\n",
    "                model = lgb.train(\n",
    "                    params=params,\n",
    "                    train_set=lgb_train,\n",
    "                    num_boost_round=10500,  # WAS 10500\n",
    "                    valid_sets=[lgb_train, lgb_valid],\n",
    "                    early_stopping_rounds=100,  # WAS 100\n",
    "                    verbose_eval=200,\n",
    "                    feval=lgb_amex_metric\n",
    "                )\n",
    "                gc.collect()\n",
    "\n",
    "            X_tr, y_tr = None, None\n",
    "            # ORIG   y_va_pred = model.predict_proba(X_va, raw_score=True)\n",
    "            y_va_pred = model.predict(X_va, raw_score=True)\n",
    "            score = amex_metric(y_va, y_va_pred)\n",
    "\n",
    "            #cv_scores[idx] = log_loss(y_test, preds)\n",
    "        return np.mean(cv_scores)\n",
    "\n",
    "    def objective(self, trial, X, y, features):\n",
    "        param_grid = {\n",
    "            \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "            \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "            \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "            \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "            \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "            \"bagging_fraction\": trial.suggest_float(\n",
    "                \"bagging_fraction\", 0.2, 0.95, step=0.1\n",
    "            ),\n",
    "            \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "            \"feature_fraction\": trial.suggest_float(\n",
    "                \"feature_fraction\", 0.2, 0.95, step=0.1\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        model = lgb.LGBMClassifier(objective=\"binary\", **param_grid)\n",
    "        model.fit(\n",
    "            X,\n",
    "            y,\n",
    "            eval_set=[(X, y)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            early_stopping_rounds=100,\n",
    "            callbacks=[\n",
    "                # LightGBMPruningCallback(trial, \"binary_logloss\")\n",
    "            ],  # Add a pruning callback\n",
    "        )\n",
    "        y_va_pred = model.predict(X_test, raw_score=True)\n",
    "\n",
    "        return np.mean(cv_scores)\n",
    "\n",
    "    def seed_everything(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    # def read_data(self):\n",
    "    #     train = pd.read_parquet(self.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    #     test = pd.read_parquet(self.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    #     return train, test\n",
    "\n",
    "    def amex_metric(y_true, y_pred):\n",
    "        # count of positives and negatives\n",
    "        n_pos = y_true.sum()\n",
    "        n_neg = y_true.shape[0] - n_pos\n",
    "\n",
    "        # sorting by descring prediction values\n",
    "        indices = np.argsort(y_pred)[::-1]\n",
    "        preds, target = y_pred[indices], y_true[indices]\n",
    "\n",
    "        # filter the top 4% by cumulative row weights\n",
    "        weight = 20.0 - target * 19.0\n",
    "        cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "        four_pct_filter = cum_norm_weight <= 0.04\n",
    "\n",
    "        # default rate captured at 4%\n",
    "        d = target[four_pct_filter].sum() / n_pos\n",
    "\n",
    "        # weighted gini coefficient\n",
    "        lorentz = (target / n_pos).cumsum()\n",
    "        gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "        # max weighted gini coefficient\n",
    "        gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "        # normalized weighted gini coefficient\n",
    "        g = gini / gini_max\n",
    "\n",
    "        return 0.5 * (g + d)\n",
    "\n",
    "    def amex_metric_np(preds, target):\n",
    "        indices = np.argsort(preds)[::-1]\n",
    "        preds, target = preds[indices], target[indices]\n",
    "        weight = 20.0 - target * 19.0\n",
    "        cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "        four_pct_mask = cum_norm_weight <= 0.04\n",
    "        d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "        weighted_target = target * weight\n",
    "        lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "        gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "        n_pos = np.sum(target)\n",
    "        n_neg = target.shape[0] - n_pos\n",
    "        gini_max = 10 * n_neg * \\\n",
    "            (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "        g = gini / gini_max\n",
    "        return 0.5 * (g + d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = create_study(direction=\"maximize\")\n",
    "\n",
    "ideal_model = IdealModel(target)\n",
    "\n",
    "study.optimize(ideal_model.objective_cross(), n_trials=20, timeout=600)\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state ==\n",
    "                 optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state ==\n",
    "                   optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing Hyperparameter Results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INFERENCE:\n",
    "    sub = pd.DataFrame({'customer_ID': test.index,\n",
    "                        'prediction': np.mean(y_pred_list, axis=0)})\n",
    "    sub.to_csv(f'{file_name}_{CV_score}', index=False)\n",
    "    display(sub)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
