{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Credits:** Based on [this](https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7963) amazing notebook.\n",
    "\n",
    "OK. Maybe not **all** you need.\n",
    "<br>\n",
    "But they improve `LightGBM`!\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thop\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torchvision\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "max_depth = range(3,12)\n",
    "\n",
    "## Num_leaves should be maximum of 2^(max_depth)\n",
    "\n",
    "## min)data_in_leaf-> number of observations that fir the decision \n",
    "## criteria\n",
    "\n",
    "# n_estimators -> number of decision trees\n",
    "# learning_rate -> step size param of gradient descent\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DIR = \"..\"\n",
    "BATCHSIZE = 128\n",
    "N_TRAIN_EXAMPLES = BATCHSIZE * 30\n",
    "N_VALID_EXAMPLES = BATCHSIZE * 10\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import random\n",
    "import warnings\n",
    "import itertools\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import combinations\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from catboost import CatBoostClassifier\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import numpy as np\n",
    "from hyperopt import tpe, hp, fmin, STATUS_OK,Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "class IdealModel:\n",
    "    def __init(self, space, type, score, folds=5, input_dir='./input/amex-fe'):\n",
    "        self.score = score\n",
    "        self.space = space\n",
    "        self.type = type\n",
    "        self.folds = folds\n",
    "        self.input_dir = input_dir\n",
    "        \n",
    "    def objective(self, trial, X, y):\n",
    "        param_grid = {\n",
    "            # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "            \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "            \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "            \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "            \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "            \"bagging_fraction\": trial.suggest_float(\n",
    "                \"bagging_fraction\", 0.2, 0.95, step=0.1\n",
    "            ),\n",
    "            \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "            \"feature_fraction\": trial.suggest_float(\n",
    "                    \"feature_fraction\", 0.2, 0.95, step=0.1\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=self.folds, shuffle=True, random_state=1121218)\n",
    "\n",
    "        cv_scores = np.empty(5)\n",
    "        for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            model = lgb.LGBMClassifier(objective=\"binary\", **param_grid)\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=[(X_test, y_test)],\n",
    "                eval_metric=\"binary_logloss\",\n",
    "                early_stopping_rounds=100,\n",
    "                callbacks=[\n",
    "                # LightGBMPruningCallback(trial, \"binary_logloss\")\n",
    "                ],  # Add a pruning callback\n",
    "            )\n",
    "            preds = model.predict_proba(X_test)\n",
    "            #cv_scores[idx] = log_loss(y_test, preds)\n",
    "\n",
    "        return np.mean(cv_scores)\n",
    "\n",
    "    def seed_everything(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    def read_data():\n",
    "        train = pd.read_parquet(IdealModel.input_dir + 'train_fe_plus_plus.parquet')\n",
    "        test = pd.read_parquet(IdealModel.input_dir + 'test_fe_plus_plus.parquet')\n",
    "        return train, test\n",
    "\n",
    "    def amex_metric(y_true, y_pred):\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "        weights = np.where(labels[:,0]==0, 20, 1)\n",
    "        cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "        top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "        gini = [0,0]\n",
    "        for i in [1,0]:\n",
    "            labels = np.transpose(np.array([y_true, y_pred]))\n",
    "            labels = labels[labels[:, i].argsort()[::-1]]\n",
    "            weight = np.where(labels[:,0]==0, 20, 1)\n",
    "            weight_random = np.cumsum(weight / np.sum(weight))\n",
    "            total_pos = np.sum(labels[:, 0] *  weight)\n",
    "            cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "            lorentz = cum_pos_found / total_pos\n",
    "            gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "        return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "    def amex_metric_np(preds, target):\n",
    "        indices = np.argsort(preds)[::-1]\n",
    "        preds, target = preds[indices], target[indices]\n",
    "        weight = 20.0 - target * 19.0\n",
    "        cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "        four_pct_mask = cum_norm_weight <= 0.04\n",
    "        d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "        weighted_target = target * weight\n",
    "        lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "        gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "        n_pos = np.sum(target)\n",
    "        n_neg = target.shape[0] - n_pos\n",
    "        gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "        g = gini / gini_max\n",
    "        return 0.5 * (g + d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, valid_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "            data, target = data.view(-1, 28 * 28).to(DEVICE), target.to(DEVICE)\n",
    "            pred = model(data).argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    accuracy = correct / N_VALID_EXAMPLES\n",
    "\n",
    "    flops, _ = thop.profile(model, inputs=(torch.randn(1, 28 * 28).to(DEVICE),), verbose=False)\n",
    "    return flops, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing Hyperparameter Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
