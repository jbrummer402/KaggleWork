{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecaf11bd",
   "metadata": {
    "papermill": {
     "duration": 0.004775,
     "end_time": "2022-07-20T15:34:04.629652",
     "exception": false,
     "start_time": "2022-07-20T15:34:04.624877",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Lag Features Are All You Need\n",
    "\n",
    "> **Credits:** Based on [this](https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7963) amazing notebook.\n",
    "\n",
    "OK. Maybe not **all** you need.\n",
    "<br>\n",
    "But they improve `LightGBM`!\n",
    "_____\n",
    "\n",
    "\n",
    "This notebook stated as an ensemble of `LightGBM` + `Catboost` + `XGB` but while running it I discovered an interestin idea that worked really well.\n",
    "\n",
    "### Lag Features\n",
    "\n",
    "On this competition we get information about clients of AMEX over time. \n",
    "Most high scoring notebooks on this competiion focused on aggregating the information per client and create a single row of extracted features: One for each client.\n",
    "\n",
    "**One of such agg function is `last`**.\n",
    "\n",
    "Quick examination revealed that the `last` feature is extreamly powerful at predicting if the client defaults or not (well.. make sense..). \n",
    "So I took this two steps further: \n",
    "\n",
    "- **First feature:** Just like the `last` feature: I added a `first` feature. \n",
    "- **\"Lag\" fearures:** to capture the change over time about each client I calculated two features for every `first`, `last` pair:\n",
    "     - **Last - First:** The change since we first see the client to the last time we see the client.\n",
    "     - **Last / First:** The fractional difference since we first see the client to the last time we see the client.\n",
    "\n",
    "This improved my `LightGBM` model to the point that it overtook the whole `LightGBM` + `Catboost` + `XGB` ensemble.\n",
    "\n",
    "I uploaded a dataset containing the extracted lag features and updated the final model predictions (only `LightGBM` this time) for everyone to play with. \n",
    "\n",
    "<br>\n",
    "\n",
    "_____\n",
    "\n",
    "**Next Experiement (currently running):** More \"lag features\" variations - Also take in consideration other indices of the time-series. will keep you updated.\n",
    "_____\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036efb2",
   "metadata": {
    "papermill": {
     "duration": 0.003494,
     "end_time": "2022-07-20T15:34:04.639605",
     "exception": false,
     "start_time": "2022-07-20T15:34:04.636111",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b725b242",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-20T15:34:04.649429Z",
     "iopub.status.busy": "2022-07-20T15:34:04.648709Z"
    },
    "papermill": {
     "duration": 256.389327,
     "end_time": "2022-07-20T15:38:21.032684",
     "exception": false,
     "start_time": "2022-07-20T15:34:04.643357",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting train feature extraction\n",
      "Train shape:  (458913, 1462)\n",
      "Starting test feature extraction\n",
      "Test shape:  (924621, 1461)\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('./input/amex-data-integer-dtypes-parquet-format/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    \n",
    "    # Train FE\n",
    "    print('Starting train feature extraction')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in train_num_agg:\n",
    "        if 'last' in col and col.replace('last', 'first') in train_num_agg:\n",
    "            train_num_agg[col + '_lag_sub'] = train_num_agg[col] - train_num_agg[col.replace('last', 'first')]\n",
    "            train_num_agg[col + '_lag_div'] = train_num_agg[col] / train_num_agg[col.replace('last', 'first')]\n",
    "\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    \n",
    "    train_labels = pd.read_csv('./input/amex-default-prediction/train_labels.csv')\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    print('Train shape: ', train.shape)    \n",
    "    del train_num_agg, train_cat_agg        \n",
    "    gc.collect()\n",
    "    \n",
    "    # Test FE\n",
    "    test = pd.read_parquet('./input/amex-data-integer-dtypes-parquet-format/test.parquet')\n",
    "    print('Starting test feature extraction')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in test_num_agg:\n",
    "        if 'last' in col and col.replace('last', 'first') in test_num_agg:\n",
    "            test_num_agg[col + '_lag_sub'] = test_num_agg[col] - test_num_agg[col.replace('last', 'first')]\n",
    "            test_num_agg[col + '_lag_div'] = test_num_agg[col] / test_num_agg[col.replace('last', 'first')]\n",
    "\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    \n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID')\n",
    "    print('Test shape: ', test.shape)\n",
    "    del test_num_agg, test_cat_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    # Save files to disk\n",
    "    train.to_parquet('train_fe_plus_plus.parquet')\n",
    "    test.to_parquet('test_fe_plus_plus.parquet')\n",
    "    \n",
    "# Read & Preprocess Data\n",
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2c10e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94ebbc49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T16:36:50.115416Z",
     "iopub.status.busy": "2022-07-03T16:36:50.115074Z",
     "iopub.status.idle": "2022-07-03T16:36:52.698333Z",
     "shell.execute_reply": "2022-07-03T16:36:52.697548Z",
     "shell.execute_reply.started": "2022-07-03T16:36:50.115388Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import random\n",
    "import warnings\n",
    "import itertools\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import combinations\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from catboost import CatBoostClassifier\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "class CFG:\n",
    "    input_dir = './input/amex-fe/'\n",
    "    seed = 42\n",
    "    n_folds = 2\n",
    "    target = 'target'\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "def amex_metric_np(preds, target):\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9912c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training LightGBM (DART) Model\n",
    "\n",
    "- Final predictions output uploaded as a public dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f897eeb0",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-03T16:36:52.701018Z",
     "iopub.status.busy": "2022-07-03T16:36:52.700595Z",
     "iopub.status.idle": "2022-07-03T16:36:52.731615Z",
     "shell.execute_reply": "2022-07-03T16:36:52.730798Z",
     "shell.execute_reply.started": "2022-07-03T16:36:52.700987Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 1823 features...\n",
      "[LightGBM] [Info] Number of positive: 59414, number of negative: 170042\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.257531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 286827\n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 1810\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258934 -> initscore=-1.051516\n",
      "[LightGBM] [Info] Start training from score -1.051516\n",
      "[500]\ttraining's binary_logloss: 0.335174\ttraining's amex_metric: 0.784349\tvalid_1's binary_logloss: 0.339092\tvalid_1's amex_metric: 0.767413\n",
      "[1000]\ttraining's binary_logloss: 0.241943\ttraining's amex_metric: 0.804912\tvalid_1's binary_logloss: 0.252219\tvalid_1's amex_metric: 0.777076\n",
      "[1500]\ttraining's binary_logloss: 0.215652\ttraining's amex_metric: 0.822008\tvalid_1's binary_logloss: 0.232621\tvalid_1's amex_metric: 0.783367\n",
      "[2000]\ttraining's binary_logloss: 0.199567\ttraining's amex_metric: 0.839969\tvalid_1's binary_logloss: 0.22439\tvalid_1's amex_metric: 0.787368\n",
      "[2500]\ttraining's binary_logloss: 0.190664\ttraining's amex_metric: 0.854052\tvalid_1's binary_logloss: 0.221895\tvalid_1's amex_metric: 0.789773\n",
      "[3000]\ttraining's binary_logloss: 0.181604\ttraining's amex_metric: 0.866939\tvalid_1's binary_logloss: 0.219882\tvalid_1's amex_metric: 0.791119\n",
      "[3500]\ttraining's binary_logloss: 0.17314\ttraining's amex_metric: 0.880575\tvalid_1's binary_logloss: 0.218588\tvalid_1's amex_metric: 0.791775\n",
      "[4000]\ttraining's binary_logloss: 0.165624\ttraining's amex_metric: 0.893515\tvalid_1's binary_logloss: 0.217804\tvalid_1's amex_metric: 0.792842\n",
      "[4500]\ttraining's binary_logloss: 0.158352\ttraining's amex_metric: 0.905225\tvalid_1's binary_logloss: 0.217236\tvalid_1's amex_metric: 0.794085\n",
      "[5000]\ttraining's binary_logloss: 0.15123\ttraining's amex_metric: 0.917354\tvalid_1's binary_logloss: 0.216797\tvalid_1's amex_metric: 0.794486\n",
      "[5500]\ttraining's binary_logloss: 0.144931\ttraining's amex_metric: 0.928024\tvalid_1's binary_logloss: 0.21656\tvalid_1's amex_metric: 0.794642\n",
      "[6000]\ttraining's binary_logloss: 0.139603\ttraining's amex_metric: 0.936813\tvalid_1's binary_logloss: 0.216396\tvalid_1's amex_metric: 0.794871\n",
      "[6500]\ttraining's binary_logloss: 0.13405\ttraining's amex_metric: 0.94426\tvalid_1's binary_logloss: 0.216222\tvalid_1's amex_metric: 0.794847\n",
      "[7000]\ttraining's binary_logloss: 0.127753\ttraining's amex_metric: 0.953076\tvalid_1's binary_logloss: 0.216092\tvalid_1's amex_metric: 0.794947\n",
      "[7500]\ttraining's binary_logloss: 0.12182\ttraining's amex_metric: 0.961955\tvalid_1's binary_logloss: 0.216045\tvalid_1's amex_metric: 0.795167\n",
      "[8000]\ttraining's binary_logloss: 0.116635\ttraining's amex_metric: 0.967931\tvalid_1's binary_logloss: 0.216093\tvalid_1's amex_metric: 0.794887\n",
      "[8500]\ttraining's binary_logloss: 0.112286\ttraining's amex_metric: 0.973416\tvalid_1's binary_logloss: 0.216101\tvalid_1's amex_metric: 0.794338\n",
      "[9000]\ttraining's binary_logloss: 0.107455\ttraining's amex_metric: 0.978653\tvalid_1's binary_logloss: 0.21615\tvalid_1's amex_metric: 0.794868\n",
      "[9500]\ttraining's binary_logloss: 0.10312\ttraining's amex_metric: 0.982932\tvalid_1's binary_logloss: 0.216209\tvalid_1's amex_metric: 0.7951\n",
      "[10000]\ttraining's binary_logloss: 0.0989658\ttraining's amex_metric: 0.986694\tvalid_1's binary_logloss: 0.216285\tvalid_1's amex_metric: 0.79475\n",
      "[10500]\ttraining's binary_logloss: 0.095384\ttraining's amex_metric: 0.98933\tvalid_1's binary_logloss: 0.216399\tvalid_1's amex_metric: 0.795039\n",
      "Our fold 0 CV score is 0.7950392334292715\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 1823 features...\n",
      "[LightGBM] [Info] Number of positive: 59414, number of negative: 170043\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.254127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 286722\n",
      "[LightGBM] [Info] Number of data points in the train set: 229457, number of used features: 1814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051521\n",
      "[LightGBM] [Info] Start training from score -1.051521\n",
      "[500]\ttraining's binary_logloss: 0.333869\ttraining's amex_metric: 0.785384\tvalid_1's binary_logloss: 0.340204\tvalid_1's amex_metric: 0.764067\n",
      "[1000]\ttraining's binary_logloss: 0.240902\ttraining's amex_metric: 0.806561\tvalid_1's binary_logloss: 0.253794\tvalid_1's amex_metric: 0.773786\n",
      "[1500]\ttraining's binary_logloss: 0.214859\ttraining's amex_metric: 0.82515\tvalid_1's binary_logloss: 0.233954\tvalid_1's amex_metric: 0.779908\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jackm\\Desktop\\Code\\KaggleWork\\AMEX_DEFAULT_PRED\\jmb-lag-features-are-all-you-need.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=92'>93</a>\u001b[0m seed_everything(CFG\u001b[39m.\u001b[39mseed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=93'>94</a>\u001b[0m train, test \u001b[39m=\u001b[39m read_data()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=94'>95</a>\u001b[0m train_and_evaluate(train, test)\n",
      "\u001b[1;32mc:\\Users\\jackm\\Desktop\\Code\\KaggleWork\\AMEX_DEFAULT_PRED\\jmb-lag-features-are-all-you-need.ipynb Cell 7\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(train, test)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=57'>58</a>\u001b[0m lgb_train \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(x_train, y_train, categorical_feature \u001b[39m=\u001b[39m cat_features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=58'>59</a>\u001b[0m lgb_valid \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(x_val, y_val, categorical_feature \u001b[39m=\u001b[39m cat_features)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=59'>60</a>\u001b[0m model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=60'>61</a>\u001b[0m     params \u001b[39m=\u001b[39;49m params,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=61'>62</a>\u001b[0m     train_set \u001b[39m=\u001b[39;49m lgb_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=62'>63</a>\u001b[0m     num_boost_round \u001b[39m=\u001b[39;49m \u001b[39m10500\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=63'>64</a>\u001b[0m     valid_sets \u001b[39m=\u001b[39;49m [lgb_train, lgb_valid],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=64'>65</a>\u001b[0m     early_stopping_rounds \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=65'>66</a>\u001b[0m     verbose_eval \u001b[39m=\u001b[39;49m \u001b[39m500\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=66'>67</a>\u001b[0m     feval \u001b[39m=\u001b[39;49m lgb_amex_metric\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=67'>68</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=68'>69</a>\u001b[0m \u001b[39m# Save best model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jackm/Desktop/Code/KaggleWork/AMEX_DEFAULT_PRED/jmb-lag-features-are-all-you-need.ipynb#ch0000006?line=69'>70</a>\u001b[0m joblib\u001b[39m.\u001b[39mdump(model, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlgbm_fold\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m_seed\u001b[39m\u001b[39m{\u001b[39;00mCFG\u001b[39m.\u001b[39mseed\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jackm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    285\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[0;32m    286\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[0;32m    287\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[0;32m    288\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[0;32m    289\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[0;32m    290\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[1;32m--> 292\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[0;32m    294\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    295\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jackm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[0;32m   3022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[0;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[0;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "def train_and_evaluate(train, test):\n",
    "    # Label encode categorical features\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ]\n",
    "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        test[col + '_round2'] = test[col].round(2)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': CFG.seed,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        'min_data_in_leaf': 40\n",
    "        }\n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 500,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, f'lgbm_fold{fold}_seed{CFG.seed}.pkl')\n",
    "        # Predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Predict the test set\n",
    "        test_pred = model.predict(test[features])\n",
    "        test_predictions += test_pred / CFG.n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train[CFG.target], oof_predictions)\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(f'oof_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(f'test_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "train, test = read_data()\n",
    "train_and_evaluate(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238bcac6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Prediction\n",
    "\n",
    "- Replace / comment-out this to use your own predictions from the model in the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc062af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T16:36:52.797315Z",
     "iopub.status.busy": "2022-07-03T16:36:52.796711Z",
     "iopub.status.idle": "2022-07-03T16:37:04.221485Z",
     "shell.execute_reply": "2022-07-03T16:37:04.220504Z",
     "shell.execute_reply.started": "2022-07-03T16:36:52.797274Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df_1 = pd.read_csv('./input/amex-predictions/test_lgbm_baseline_5fold_seed42.csv')\n",
    "df_1.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 268.117077,
   "end_time": "2022-07-20T15:38:21.081247",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-20T15:33:52.964170",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "bea2f145059fa3dfc9a4373fa52e8ba84bfcc9279a5a4281ca435068bf44c804"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
